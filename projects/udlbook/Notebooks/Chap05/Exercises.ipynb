{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67738997-7798-41ac-a7d8-abefb358d6d0",
   "metadata": {},
   "source": [
    "### Problem 5.1 Show that the logistic sigmoid function sig[z] becomes 0 as z → −∞, is 0.5 when z = 0, and becomes 1 when z → ∞, where: sig[z] = 1 1 + exp[−z]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26904ec6-0eca-4d8d-a0a3-ad69c133d861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T23:57:07.921932Z",
     "iopub.status.busy": "2024-07-27T23:57:07.921493Z",
     "iopub.status.idle": "2024-07-27T23:57:07.927432Z",
     "shell.execute_reply": "2024-07-27T23:57:07.926334Z",
     "shell.execute_reply.started": "2024-07-27T23:57:07.921904Z"
    }
   },
   "source": [
    "\\begin{align}\n",
    "    sig[z] = \\frac{1}{1+exp[-z]}\n",
    "\\end{align}\n",
    "\n",
    "As $z \\rightarrow -\\infty$ then $exp[-z] \\rightarrow \\infty$ and $sig[z] \\rightarrow \\frac{1}{1 + \\infty} = 0$\n",
    "\n",
    "When $z=0, sig[z] = \\frac{1}{1+exp[0]} = \\frac{1}{1+1} = 0.5$\n",
    "\n",
    "As $z \\rightarrow \\infty$, $exp[-z] \\rightarrow 0$, so $sig[z] \\rightarrow \\frac{1}{1 + 0} = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfd283-180d-43eb-a187-55a95f1c8237",
   "metadata": {},
   "source": [
    "### Problem 5.2 The loss L for binary classification for a single training pair {x, y} is: L = −(1 − y) logh 1 − sig[f[x, ϕ]]i − y logh sig[f[x, ϕ]]i , (5.33) where sig[•] is defined in equation 5.32.  lot this loss as a function of the transformed network output sig[f[x, ϕ]] ∈ [0, 1] (i) when the training label y = 0 and (ii) when y = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb61ab-650b-4c51-a131-5a8f1faed337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "eps = 1e-6\n",
    "n_out = np.linspace(0 + eps, 1 - eps, 100)\n",
    "\n",
    "def cross_entropy(pred, y):\n",
    "    return (1 - y)*np.log(1 - pred) - y*np.log(pred)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 5), ncols=2)\n",
    "\n",
    "for y, ax in enumerate(axes):\n",
    "    ax.plot(n_out, cross_entropy(n_out, y))\n",
    "    ax.set_title(f\"y = {y}\", fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9639c-02ac-42a3-8af9-a52a3a4a3f84",
   "metadata": {},
   "source": [
    "### Problem 5.3∗ Suppose we want to build a model that predicts the direction y in radians of the prevailing wind based on local measurements of barometric pressure x. A suitable distribution over circular domains is the von Mises distribution (figure 5.13): P r(y|µ, κ) = exp κ cos[y − µ] 2π · Bessel0[κ],where µ is a measure of the mean direction and κ is a measure of concentration (i.e., the inverse of the variance). The term Bessel0[κ] is a modified Bessel function of the first kind of order 0. Use the recipe from section 5.2 to develop a loss function for learning the parameter µ of a model f[x, ϕ] to predict the most likely wind direction. Your solution should treat the concentration κ as constant. How would you perform inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09b71b-da35-45c1-8b6e-b780ab42c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import j0  # Bessel function of the first kind, order 0\n",
    "# 1. Choose our distribution\n",
    "\n",
    "def p_von_mises(y, mu, kappa):\n",
    "    return np.exp(kappa * np.cos(y - mu)) / (2 * np.pi * j0(kappa))\n",
    "\n",
    "# 2. Make a model\n",
    "def model(x):\n",
    "    return 2 * np.pi * x\n",
    "\n",
    "# 3. compute the nll\n",
    "def nll(y, mu, k):\n",
    "    return -np.sum(np.log(p_von_mises(y, mu, k)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d8386-25de-4b69-ad44-b54f022f43ee",
   "metadata": {},
   "source": [
    "Essentially, we train a model to predict mu from surface pressure x by minimizing the nll over the data y and the predicted mu, holding kappa constant.  \n",
    "In this situation, inference is straightforward, as we just return the model prediction.\n",
    "\n",
    "If we don't care about the uncertainty as a trainable parameter we can simplify the loss function by pushing the log through the von Mises distribution\n",
    "\n",
    "\\begin{align}\n",
    "    \\log (P_{vM}(y | \\mu, \\kappa)) &= \\kappa cos(y - \\mu) - \\log (2\\pi Bessel_0(\\kappa)) \\\\    \n",
    "\\end{align}\n",
    "\n",
    "And we can throw away the bessel term since we are not training our model on $\\kappa$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62986f78-0bfd-4b38-a161-88b626dde565",
   "metadata": {},
   "source": [
    "### Problem 5.4∗ Sometimes, the outputs y for input x are multimodal (figure 5.14a); there is more than one valid prediction for a given input. Here, we might use a weighted sum of normal components as the distribution over the output. This is known as a mixture of Gaussians model. For example, a mixture of two Gaussians has parameters θ = {λ, µ1, σ12 1, µ2, σ22}: P r(y|λ, µ1, µ2, σ21, σ22) = λp2πσ21 exp \u0014−(y − µ1)22σ21\u0015 + 1 − λp2πσ22exp \u0014−(y − µ2)22σ22\u0015, (5.35) where λ ∈ [0, 1] controls the relative weight of the two components, which have means µ1, µ2and variances σ21, σ22, respectively. This model can represent a distribution with two peaks (figure 5.14b) or a distribution with one peak but a more complex shape (figure 5.14c).Use the recipe from section 5.2 to construct a loss function for training a model f[x, ϕ] that takes input x, has parameters ϕ, and predicts a mixture of two Gaussians. The loss should be based on I training data pairs {xi, yi}. What problems do you foresee when performing inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee7f96-f3ba-472f-ba5f-23d9421b36aa",
   "metadata": {},
   "source": [
    "For each data point pair, we're going to get a pretty gross expression\n",
    "\n",
    "\\begin{align}\n",
    "    L[\\theta] = -\\sum\\limits_i \\log (\\lambda[x_i, \\phi] P(y_i | \\mu_1[x_i, \\phi], \\sigma_1[x_i, \\phi]) + (1 - \\lambda[x_i, \\phi]) P(y_i | \\mu_2[x_i, \\phi], \\sigma_2[x_i, \\phi]))\n",
    "\\end{align}\n",
    "\n",
    "Because log doesn't distribute over addition, theres going to be no easy way to get a nice expression for the loss, which in turn means we're going to have to sample from the model distribution to get the MAP estimate.  This is fine, just more expensive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afadf6d-7d86-4120-8661-20587c4da588",
   "metadata": {},
   "source": [
    "### Problem 5.5 Consider extending the model from problem 5.3 to predict the wind direction using a mixture of two von Mises distributions. Write an expression for the likelihood P r(y|θ) for this model. How many outputs will the network need to produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316506b1-e2ab-4ac6-9709-8ecbea3ddb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:57:42.764301Z",
     "iopub.status.busy": "2024-07-28T14:57:42.763872Z",
     "iopub.status.idle": "2024-07-28T14:57:42.771425Z",
     "shell.execute_reply": "2024-07-28T14:57:42.770404Z",
     "shell.execute_reply.started": "2024-07-28T14:57:42.764269Z"
    }
   },
   "source": [
    "\\begin{align}\n",
    "    \\theta &= \\{\\lambda, \\mu_1, \\mu_2\\} \\\\\n",
    "    P(y_i | \\theta) &= \\lambda[x_i, \\phi] \\frac{\\exp[\\kappa \\cdot \\cos[y_i - \\mu_1[x_i, \\phi]]]}{2\\pi J_0[\\kappa]} + (1 - \\lambda[x_i, \\phi])  \\frac{\\exp[\\kappa \\cdot \\cos[y_i - \\mu_2[x_i, \\phi]]]}{2\\pi J_0[\\kappa]}\n",
    "\\end{align}\n",
    "\n",
    "and has three outputs to produce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca510c-0e8d-4e1c-9439-3bf8b53a4e11",
   "metadata": {},
   "source": [
    "### Problem 5.6 Consider building a model to predict the number of pedestrians y ∈ {0, 1, 2, . . .} that will pass a given point in the city in the next minute, based on data x that contains information about the time of day, the longitude and latitude, and the type of neighborhood. A suitable distribution for modeling counts is the Poisson distribution (figure 5.15). This has a single parameter λ > 0 called the rate that represents the mean of the distribution. The distribution has probability density function: P r(y = k) = λke−λk!. (5.36) Design a loss function for this model assuming we have access to I training pairs {xi, yi}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e314b-2bc1-489c-9072-7990ecb6af1a",
   "metadata": {},
   "source": [
    "The likelihood of an individual example would be\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta &= \\{ \\lambda \\} \\\\\n",
    "    P(y_i | \\theta) &= \\frac{\\lambda[x_i, \\phi]^{y_i}e^{-\\lambda[x_i, \\phi]]}}{y_i!} \\\\\n",
    "\\end{align}\n",
    "\n",
    "So our loss is \n",
    "\n",
    "\\begin{align}\n",
    "    L[\\phi] &= - \\sum\\limits_i \\log\\left[\\frac{\\lambda[x_i, \\phi]^{y_i}e^{-\\lambda[x_i, \\phi]]}}{y_i!}\\right] \\\\    \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f2fcc-40b6-4832-8550-b44d56bc50d0",
   "metadata": {},
   "source": [
    "### Problem 5.7 Consider a multivariate regression problem where we predict ten outputs, so y ∈ R10, and model each with an independent normal distribution where the means µd are predicted by the network, and variances σ2 are constant. Write an expression for the likelihood P r(y|f[x, ϕ]). Show that minimizing the negative log-likelihood of this model is still equivalent to minimizing a sum of squared terms if we don’t estimate the variance σ2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d94d7b-7a7e-449a-bf3a-5b65afd8178d",
   "metadata": {},
   "source": [
    "We'd do this with a multivariate Gaussian distribution\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta &= \\{ \\mathbf{\\mu} \\} \\\\\n",
    "    P(\\mathbf{y}_i | \\theta) &= \\frac{1}{\\sqrt{(2\\pi)^{10} |\\Sigma |}} \\exp\\left[ -\\frac{1}{2}(\\mathbf{y}_i - \\mathbf{\\mu}[\\mathbf{x_i}, \\phi])^T \\Sigma^-1 (\\mathbf{y}_i - \\mathbf{\\mu}[\\mathbf{x_i}, \\phi])\\right]\n",
    "\\end{align}\n",
    "\n",
    "Which produces a loss (after throwing away some terms constant w/r/t minimization)\n",
    "\n",
    "\\begin{align}\n",
    "    L[\\phi] &= \\sum\\limits_i (\\mathbf{y}_i - \\mathbf{\\mu}[\\mathbf{x_i}, \\phi])^T \\Sigma^-1 (\\mathbf{y}_i - \\mathbf{\\mu}[\\mathbf{x_i}, \\phi])\n",
    "\\end{align}\n",
    "\n",
    "Assuming each output is independent means $\\Sigma$ is diagonal and we can rewrite this as \n",
    "\n",
    "\\begin{align}\n",
    "    L[\\phi] &= \\sum\\limits_i \\sum\\limits_j^{10} \\frac{(y_{ij} - \\mu_j[x_{ij}, \\phi])^2}{\\sigma_j^2} \\\\\n",
    "    &\\rightarrow \\sum\\limits_i \\sum\\limits_j^{10} (y_{ij} - \\mu_j[x_{ijk}, \\phi])^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $j$ ranges over the dimensions of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c627df-6d16-4392-b8bd-8c8b3d4e8375",
   "metadata": {},
   "source": [
    "### Problem 5.8∗ Construct a loss function for making multivariate predictions y ∈ R Do based on independent normal distributions with different variances σ2d for each dimension. Assume a heteroscedastic model so that both the means µd and variances σ2 d vary as a function of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544da20-b639-4b49-883b-e0a4ac5177ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T15:53:50.219294Z",
     "iopub.status.busy": "2024-07-28T15:53:50.218637Z",
     "iopub.status.idle": "2024-07-28T15:53:50.224915Z",
     "shell.execute_reply": "2024-07-28T15:53:50.223914Z",
     "shell.execute_reply.started": "2024-07-28T15:53:50.219270Z"
    }
   },
   "source": [
    "The likelihood for an individual example is the same as in 5.7, but we can no longer throw away the normalization term.  Let's first rewrite as a product of distributions (from independence)\n",
    "\\begin{align}\n",
    "    \\theta &= \\{ \\mu_j, \\sigma_j \\} \\\\\n",
    "    P(\\mathbf{y}_i | \\theta) &= \\prod\\limits_j \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}}  \\exp\\left[ -\\frac{ y_{ij} - \\mu_j}{2\\sigma_j^2} \\right]\n",
    "\\end{align}\n",
    "\n",
    "Which produces the loss\n",
    "\n",
    "\\begin{align}\n",
    "    L[\\phi] &= -\\sum\\limits_i log\\left[ \\prod\\limits_j \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}}  \\exp\\left[ -\\frac{ y_{ij} - \\mu_j}{2\\sigma_j^2} \\right]\\right] \\\\\n",
    "    &= \\sum\\limits_i \\sum\\limits_j \\pi \\sigma_j^2 + \\frac{(y_{ij} - \\mu_j)^2}{\\sigma_j^2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbf53e-ae71-43f1-a395-21332066034c",
   "metadata": {},
   "source": [
    "### Problem 5.9∗ Consider a multivariate regression problem in which we predict the height of a person in meters and their weight in kilos from data x. Here, the units take quite different ranges. What problems do you see this causing? Propose two solutions to these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37804da3-6462-4ad6-8106-d87f342ed0d3",
   "metadata": {},
   "source": [
    "Because the two outputs will come in very different units, the loss will pay more attention to one than the other.  Two approaches to this problem:\n",
    "\n",
    "1. Predict a z-score for height and weight (ie standard deviations from some fixed mean).  This will put things in the same scale by definition.\n",
    "2. Learn the scale of the data with an additional parameter for each output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf078b8-cd88-46f0-9245-d66f1825b413",
   "metadata": {},
   "source": [
    "### Problem 5.10 Extend the model from problem 5.3 to predict both the wind direction and the wind speed and define the associated loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8084a-dcb8-4dfb-b9d6-e8a0e4003c75",
   "metadata": {},
   "source": [
    "If we assume wind direction and wind speed are independent conditioned on the predictors, then we can write a likelihood for a data point using a product of two distributions. We'll keep the von Mises distribution with constant concentration for direction $\\psi$ and use a gamma distribution for wind speed $v$, giving us:\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta &= \\{\\mu, \\alpha, \\beta\\} \\\\\n",
    "    P(\\psi_i, v_i | \\theta) &= \\frac{\\exp[\\kappa \\cdot \\cos[\\psi_i - \\mu]]}{2\\pi J_0[\\kappa]} \\frac{\\beta^{\\alpha}}{\\Gamma[\\alpha]}v_i^{\\alpha-1}e^{-\\beta v_i}\n",
    "\\end{align}\n",
    "\n",
    "yielding the likelihood (dropping irrelevant terms)\n",
    "\n",
    "\\begin{align}\n",
    "    L[\\phi] = - \\sum\\limits_i \\kappa \\cdot \\cos[\\psi_i - \\mu[x_i, \\phi]]] + \\log\\left[\\frac{\\beta[x_i, \\phi]^{\\alpha[x_i, \\phi]}}{\\Gamma[\\alpha[x_i, \\phi]]}v_i^{\\alpha[x_i, \\phi]-1}e^{-\\beta[x_i, \\phi] v_i}\\right] \\\\\n",
    "    = - \\sum\\limits_i \\kappa \\cdot \\cos[\\psi_i - \\mu[x_i, \\phi]]] + \\alpha[x_i, \\phi]\\log[\\beta[x_i, \\phi]] - \\log[\\Gamma[\\alpha[x_i, \\phi]]] + (\\alpha[x_i, \\phi] - 1) \\log[v_i] - \\beta[x_i, \\phi]v_i\n",
    "\\end{align}\n",
    "\n",
    "Which sucks, but is fine, I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc6c74-8290-48f6-9c32-abe7e0ca65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
