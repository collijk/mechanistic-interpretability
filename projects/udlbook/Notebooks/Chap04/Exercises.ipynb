{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49beb076-a254-405e-b3da-d6eba912ac3b",
   "metadata": {},
   "source": [
    "### Problem 4.1∗ Consider composing the two neural networks in figure 4.8. Draw a plot of the relationship between the input x and output y′ for x ∈ [−1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d30317-b1f0-4f16-82c5-2393af865ae2",
   "metadata": {},
   "source": [
    "Drew separately, looks like two plateaus and a sharp peak in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06023140-cb84-4bfa-8850-6f428878b15c",
   "metadata": {},
   "source": [
    "### Problem 4.2 Identify the four hyperparameters in figure 4.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52affc83-6c1c-4b47-887f-f01612932eaa",
   "metadata": {},
   "source": [
    "1. K - the network depth\n",
    "2. D_1 - Width of the first layer\n",
    "3. D_2 - Width of the second layer\n",
    "4. D_3 - Width of the third layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07b24d-1ae8-4388-92b0-512cd2b4d691",
   "metadata": {},
   "source": [
    "### Problem 4.3 Using the non-negative homogeneity property of the ReLU function (see problem 3.5), show that: ReLUh β1+λ1 ·Ω1ReLU [β0+λ0 · Ω0x] i =λ0λ1 · ReLU \u0014 1 λ0λ1 β1+Ω1ReLU \u0014 1 λ0 β0+Ω0x \u0015\u0015 , (4.18) where λ0 and λ1 are non-negative scalars. From this, we see that the weight matrices can be rescaled by any magnitude as long as the biases are also adjusted, and the scale factors can  be re-applied at the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4964f-8440-4e6c-945e-24d86cd8f7bc",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    & ReLU[\\beta_1 + \\lambda_1\\cdot\\mathbf{\\Omega}_1 ReLU[\\beta_0 + \\lambda_0 \\cdot \\mathbf{\\Omega}_0\\mathbf{x}]] \\\\\n",
    "    &= ReLU[\\beta_1 + \\lambda_1\\cdot\\mathbf{\\Omega}_1 ReLU[\\lambda_0 \\cdot(\\frac{\\beta_0}{\\lambda_0} + \\mathbf{\\Omega}_0\\mathbf{x})]] \\\\\n",
    "    &= ReLU[\\beta_1 + \\lambda_0\\lambda_1\\cdot\\mathbf{\\Omega}_1 ReLU[\\frac{\\beta_0}{\\lambda_0} + \\mathbf{\\Omega}_0\\mathbf{x}]] \\\\\n",
    "    &= ReLU[\\lambda_0\\lambda_1\\cdot(\\frac{\\beta_1}{\\lambda_0\\lambda_1} + \\mathbf{\\Omega}_1 ReLU[\\frac{\\beta_0}{\\lambda_0} + \\mathbf{\\Omega}_0\\mathbf{x}]] \\\\\n",
    "    &= \\lambda_0\\lambda_1 \\cdot ReLU[(\\frac{\\beta_1}{\\lambda_0\\lambda_1} + \\mathbf{\\Omega}_1 ReLU[\\frac{\\beta_0}{\\lambda_0} + \\mathbf{\\Omega}_0\\mathbf{x}]] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ff29d-115d-40b8-8bee-68a79ba3fe79",
   "metadata": {},
   "source": [
    "### Problem 4.4 Write out the equations for a deep neural network that takes Di = 5 inputs, Do = 4 outputs and has three hidden layers of sizes D1 = 20, D2 = 10, and D3 = 7, respectively, in both the forms of equations 4.15 and 4.16. What are the sizes of each weight matrix Ω• and bias vector β•?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2783de7-adcd-4791-bb12-43dc23e93d03",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\mathbf{h}_1 &= a[\\mathbf{\\beta}_0 + \\mathbf{\\Omega}_0 \\mathbf{x}] \\\\\n",
    "    \\mathbf{h}_2 &= a[\\mathbf{\\beta}_1 + \\mathbf{\\Omega}_1 \\mathbf{h}_1] \\\\\n",
    "    \\mathbf{h}_3 &= a[\\mathbf{\\beta}_2 + \\mathbf{\\Omega}_2 \\mathbf{h}_2] \\\\\n",
    "    \\mathbf{y} &= a[\\mathbf{\\beta}_3 + \\mathbf{\\Omega}_3 \\mathbf{h}_3] \\\\\n",
    "\\end{align}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{y} &= a[\\mathbf{\\beta}_3 + \\mathbf{\\Omega}_3 a[\\mathbf{\\beta}_2 + \\mathbf{\\Omega}_2 a[\\mathbf{\\beta}_1 + \\mathbf{\\Omega}_1 a[\\mathbf{\\beta}_0 + \\mathbf{\\Omega}_0 \\mathbf{x}]]]]\n",
    "\\end{align}\n",
    "    \n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "    &dim(\\beta_0) = (20, 1); &dim(\\Omega_0) = (20, 5) \\\\\n",
    "    &dim(\\beta_1) = (10, 1); &dim(\\Omega_1) = (10, 20) \\\\\n",
    "    &dim(\\beta_2) = (7, 1); &dim(\\Omega_2) = (7, 10) \\\\\n",
    "    &dim(\\beta_3) = (4, 1); &dim(\\Omega_3) = (4, 7) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3f09b-97d4-432e-ab03-5de93494a9a4",
   "metadata": {},
   "source": [
    "### Problem 4.5 Consider a deep neural network with Di = 5 inputs, Do = 1 output, and K = 20 hidden layers containing D = 30 hidden units each. What is the depth of this network? What is the width?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df7367-a0ba-4837-b22f-de776e531e21",
   "metadata": {},
   "source": [
    "The depth is K = 20, the width is D = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebc4d7-4ae3-4b62-9f43-a1bd1d46b60c",
   "metadata": {},
   "source": [
    "### Problem 4.6 Consider a network with Di = 1 input, Do = 1 output, and K = 10 layers, with D = 10 hidden units in each. Would the number of weights increase more if we increased the depth by one or the width by one? Provide your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95dbe08-7cf5-4b85-85dc-a523b13109de",
   "metadata": {},
   "source": [
    "Input parameters = $D * (D_i + 1) = 10 * 2 = 20$\n",
    "\n",
    "Hidden parameters = $(K - 1) * D * (D + 1) = (10 - 1) * 10 * (10 + 1) = 990$\n",
    "\n",
    "Output paramters = $D_o * (D + 1) = 1 * (10 + 1) = 11$\n",
    "\n",
    "Total Parameters = $D * (D_i + 1) + (K - 1) * D * (D + 1) + D_o * (D + 1)$\n",
    "\n",
    "So increasing the width by one increases the number of parameters by $(D_i + 1) + (K-1)*2*(D+1) + D_o = 2 + 9*2*11 + 1 = 201$ While increasing the depth increases the number of parameters by $D(D+1) = 110$.  \n",
    "\n",
    "Throwing away the biases and the input and output layers, we can think sort of heuristically about this.  If I have K + 1 layers each with width D, we end up with K DxD matrices of weights linking the hidden layers. Adding a layer adds an additional DxD matrix worth of parameters.  Increasing the width adds ~2 * D * K  new parameters as each DxD matrix becomes a (D+1)x(D+1) matrix and adds 2D + 1 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea13bf-e568-4102-93f1-cd96bea07967",
   "metadata": {},
   "source": [
    "### Problem 4.7 Choose values for the parameters ϕ = {ϕ0, ϕ1, ϕ2, ϕ3, θ10, θ11, θ20, θ21, θ30, θ31} for the shallow neural network in equation 3.1 (with ReLU activation functions) that will define an identity function over a finite range x ∈ [a, b]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f559709-f585-4dfe-8d78-db0ea4bf7aed",
   "metadata": {},
   "source": [
    "$y = \\phi_0 + \\phi_1 a[\\theta_{10} + \\theta{11}x] + \\phi_2 a[\\theta_{20} + \\theta{21}x]+ \\phi_3 a[\\theta_{30} + \\theta{31}x]$\n",
    "\n",
    "Many ways to do this.  One easy way, set:\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi_0 &= min(x) \\\\\n",
    "    \\theta_{11} &> 0 \\\\\n",
    "    \\phi_1 &= \\theta_{11} \\\\\n",
    "    \\theta_{10} &= -min(x)\\cdot\\theta_{11}\\\\\n",
    "\\end{align}\n",
    "\n",
    "and set all the other parameters to 0.  This will, in the first hidden unit, shift x up so it's minimum is 0 and then add back the minimum when computing the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3beecc7-10fb-413c-b19d-d36310338885",
   "metadata": {},
   "source": [
    "### Problem 4.8∗ Figure 4.9 shows the activations in the three hidden units of a shallow network (as in figure 3.3). The slopes in the hidden units are 1.0, 1.0, and -1.0, respectively, and the “joints” in the hidden units are at positions 1/6, 2/6, and 4/6. Find values of ϕ0, ϕ1, ϕ2, and ϕ3 that will combine the hidden unit activations as ϕ0 + ϕ1h1 + ϕ2h2 + ϕ3h3 to create a function with four linear regions that oscillate between output values of zero and one. The slope of the leftmost region should be positive, the next one negative, and so on. How many linear regions will we create if we compose this network with itself? How many will we create if we compose it with itself K times?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad025c-19e8-4232-890e-d71186a55c10",
   "metadata": {},
   "source": [
    "First, working out the parameters using the relationships from Ex. 3.3:\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta_{10} &= -1/6 \\\\\n",
    "    \\theta_{11} &= 1 \\\\\n",
    "    \\theta_{20} &= -1/3 \\\\\n",
    "    \\theta_{21} &= 1 \\\\\n",
    "    \\theta_{30} &= 2/3 \\\\\n",
    "    \\theta_{31} &= -1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In the first region, only the third unit is active.  Let's have y intersect -1 at x=0 (we have some freedom here).  This means at $x = 0$ we have\n",
    "\n",
    "$\\phi_0 + \\phi_3 h_3 = \\phi_3 a[2/3] = \\phi_0 + \\phi_3 * 2 / 3 = -1$.  \n",
    "\n",
    "At our first joint, $x = 1/6$ and $y = 1$ so we want \n",
    "\n",
    "$\\phi_0 + \\phi_3 h_3 = \\phi_3 a[2/3 - 1/6] = \\phi_0 + \\phi_3 * 1/2 = 1$.  \n",
    "\n",
    "Subtracting these two we get:\n",
    "\n",
    "$\\phi_3 * 2/3 - \\phi_3 * 1/2 = -2 \\implies \\phi_3 = -12$\n",
    "\n",
    "and subbing back into the first equation:\n",
    "\n",
    "$\\phi_0 + 12 * 2/3 = -1 \\implies \\phi_0 = -9$\n",
    "\n",
    "At $x = 1/3$, unit 1 is also active active, so the following must be true:\n",
    "\n",
    "$\\phi_0 + \\phi_1 h_1 + \\phi_3 h_3 = 1$\n",
    "\n",
    "So subbing in everything we know we have\n",
    "\n",
    "$-9 + \\phi_1 a[-1/6 + 1 * 1/3] - 12 * a[2/3 - 1 * 1/3] = -9 + 1/6 * \\phi_1 - 12 * 1/3 = 1/6 * \\phi_1 - 13 = -1 \\implies \\phi_1 = 72$\n",
    "\n",
    "At $x = 2/3$ units 1 and 2 are active so\n",
    "\n",
    "$\\phi_0 + \\phi_1 h_1 + \\phi_2 h_2 = -1$\n",
    "\n",
    "Which means \n",
    "\n",
    "$-9 + 72 a[-1/6 + 1 * 2/3] + \\phi_2 * a[-1/3 + 1 * 2/3] = -9 + 72 * 1/2 + \\phi_2 * 1/3 = 1/3 * \\phi_2 + 27 = -1 \\implies \\phi_2 = - 84$ \n",
    "\n",
    "Then at $x = 1$ we have \n",
    "\n",
    "$-9 + 72 a[-1/6 + 1] - 84 * a[-1/3 + 1] = -9 + 72 * 5/6  - 84 * 2/3 = -9 + 60 - 56 =$ not the right answer.  I read the question incorrectly, but the same idea applies. \n",
    "\n",
    "The shape of this function means the network composed with itself will give 4*4 = 16 regions.  Composed K times will give $4^K$ regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5114027-0a37-4a4a-a85f-bf20a3b5285a",
   "metadata": {},
   "source": [
    "### Problem 4.9∗ Following problem 4.8, is it possible to create a function with three linear regions that oscillates back and forth between output values of zero and one using a shallow network with two hidden units? Is it possible to create a function with five linear regions that oscillates in the same way using a shallow network with four hidden units?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b11cf0-8608-4f1c-963c-e0eb9281b1cf",
   "metadata": {},
   "source": [
    "With two units, we can have three regions, but those three regions can only change direction once, so back and forth oscillation is not possible. With five regions and four units, we can definitely have the same oscillatory behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8c84b-c408-4250-9015-38d3cd8ec0d5",
   "metadata": {},
   "source": [
    "### Problem 4.10 Consider a deep neural network with a single input, a single output, and K hidden layers, each of which contains D hidden units. Show that this network will have a total of 3D + 1 + (K − 1)D(D + 1) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff530bf-75f2-4782-b3cd-3c5f967deedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T01:52:57.721460Z",
     "iopub.status.busy": "2024-07-26T01:52:57.721079Z",
     "iopub.status.idle": "2024-07-26T01:52:57.727674Z",
     "shell.execute_reply": "2024-07-26T01:52:57.726790Z",
     "shell.execute_reply.started": "2024-07-26T01:52:57.721421Z"
    }
   },
   "source": [
    "As above\n",
    "\n",
    "Total Parameters = $D * (D_i + 1) + (K - 1) * D * (D + 1) + D_o * (D + 1)$\n",
    "\n",
    "so in this case:\n",
    "\n",
    "Total Parameters = $D * (1 + 1) + (K - 1) * D * (D + 1) + 1 * (D + 1) = 3D + 1 + (K - 1) * D * (D + 1)$ as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e468023-d1cb-40d0-9b1e-70ab1bb316b1",
   "metadata": {},
   "source": [
    "### Problem 4.11∗ Consider two neural networks that map a scalar input x to a scalar output y. The first network is shallow and has D = 95 hidden units. The second is deep and has K = 10 layers, each containing D = 5 hidden units. How many parameters does each network have? How many linear regions can each network make (see equation 4.17)? Which would run faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae05ee9-c217-4b2b-b255-33c4b11fb4f6",
   "metadata": {},
   "source": [
    "Network 1 has $3*95 +1 = 286$ parameters.  Network 2 has $3*5 + 1 + 9*5*6=196$ parameters.  Network 1 will have a maximum of 96 linear regions $({95 \\choose 0} + {95 \\choose 1})$. Network 2 will have a maximum of \n",
    "\n",
    "\\begin{align}\n",
    "    &(5 + 1)^{10-1}({5 \\choose 0} + {5 \\choose 1}) \\\\\n",
    "    &= 6^{10} \\\\\n",
    "    &= 60466176 \\\\\n",
    "\\end{align}\n",
    "\n",
    "linear regions.  The shallow network almost certainly runs faster (assuming no memory constraints, which, like, these are small networks, so their shouldn't be any) as it has one parallel operation to perform, and the deep network needs to perform several sequential operations.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a16dfa-c9ad-49e2-a545-c5742df22ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
