{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a78ab2-417b-461e-b4fe-39fe9e983102",
   "metadata": {},
   "source": [
    "### Problem 3.1 What kind of mapping from input to output would be created if the activation function in equation 3.1 was linear so that a[z] = ψ0 + ψ1z? What kind of mapping would be created if the activation function was removed, so a[z] = z?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea42513-ac17-45f5-87ca-cb91f456534b",
   "metadata": {},
   "source": [
    "Both of these would create a linear mapping.  Removing the activation function just leaves you with the linear part:\n",
    "\n",
    "$h = a[\\phi_0 + \\phi_1 x] = \\phi_0 + \\phi_1 x$\n",
    "\n",
    "Linear functions compose so in the linear activation case we get\n",
    "\n",
    "$h = a[\\phi_0 + \\phi_1 x] = \\psi_0 + \\psi_1(\\phi_0 + \\phi_1 x) = \\phi^*_0 + \\phi^*_1 x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfd3de-0cfa-4775-a63f-e6e58aa350aa",
   "metadata": {},
   "source": [
    "### Problem 3.2 For each of the four linear regions in figure 3.3j, indicate which hidden units are inactive and which are active (i.e., which do and do not clip their inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c323bb-d851-43be-9dc5-d90c06536b15",
   "metadata": {},
   "source": [
    "1. 3\n",
    "2. 1 and 3\n",
    "3. 1, 2, and 3\n",
    "4. 1 and 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e95eb-8f55-4638-bf1d-2c05033677c9",
   "metadata": {},
   "source": [
    "### Problem 3.3∗ Derive expressions for the positions of the “joints” in function in figure 3.3j in terms of the ten parameters ϕ and the input x. Derive expressions for the slopes of the four linear regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cc594-7893-426a-9e03-65857948da7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T19:58:09.584327Z",
     "iopub.status.busy": "2024-07-21T19:58:09.584042Z",
     "iopub.status.idle": "2024-07-21T19:58:09.589174Z",
     "shell.execute_reply": "2024-07-21T19:58:09.588118Z",
     "shell.execute_reply.started": "2024-07-21T19:58:09.584300Z"
    }
   },
   "source": [
    "The preactivations are given by\n",
    "\n",
    "\\begin{align}\n",
    "    z_i &= \\theta_{i0} + \\theta_{i1}x \\\\    \n",
    "\\end{align}\n",
    "\n",
    "Joints occur where units activate, ie, where z = 0.  So we can get expressions for the joints $x^*$ as \n",
    "\n",
    "\\begin{align}\n",
    "    x^*_i &= -\\frac{\\theta_{i0}}{\\theta_{i1}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In a particular region, the output is given by\n",
    "\n",
    "\\begin{align}\n",
    "    y &= \\phi_0 + \\phi_1 h_1 + \\phi_2\\ h_2 + \\phi_3 h_3 \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can compute the slope in a region by expanding out and ignoring terms that only contribute to the intercept\n",
    "\n",
    "\\begin{align}\n",
    "    \\phi_1 h_1 + \\phi_2\\ h_2 + \\phi_3 h_3 &= \\phi_1 * a[z_1] + \\phi_2 * a[z_2] + \\phi_3 * a[z_3]\\\\\n",
    "    &= \\sum \\phi_i * max(0, \\theta_{i0} + \\theta_{i1}x)\\\\\n",
    "    &~= \\sum max(0, \\phi_i \\theta_{i1} x)\\\\\n",
    "\\end{align}\n",
    "\n",
    "Which tells us that the slope in a region is the sum of the product $\\phi_i \\theta_{i1}$ over all $i$ units active in the region.  Specifically:\n",
    "\n",
    "1. $\\phi_3 \\theta_{31}$\n",
    "2. $\\phi_1 \\theta_{11} + \\phi_3 \\theta_{31}$\n",
    "3. $\\phi_1 \\theta_{11} + \\phi_2 \\theta_{21} + \\phi_3 \\theta_{31}$\n",
    "4. $\\phi_1 \\theta_{11} + \\phi_2 \\theta_{21}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497ae6a-5692-4096-b945-ae1940ea8226",
   "metadata": {},
   "source": [
    "### Problem 3.4 Draw a version of figure 3.3 where the y-intercept and slope of the third hidden unit have changed as in figure 3.14c. Assume that the remaining parameters remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc9bf1-a23a-4d7f-8663-a3fa719484ee",
   "metadata": {},
   "source": [
    "Skipping.  This is similar manipulation to notebook 3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2c43f-2a16-4c94-9907-c1fb61bfe2a8",
   "metadata": {},
   "source": [
    "### Problem 3.5 Prove that the following property holds for α ∈ R+: ReLU[α · z] = α · ReLU[z]. (3.14) This is known as the non-negative homogeneity property of the ReLU function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaaa7c7-2e55-4020-861b-18bd9103b4a7",
   "metadata": {},
   "source": [
    "Let's rewrite:\n",
    "\n",
    "\\begin{align}\n",
    "    ReLU[z] &= max(0, z) \\\\\n",
    "    &= \\frac{z + |z|}{2}\n",
    "\\end{align}\n",
    "\n",
    "From which we can see\n",
    "\n",
    "\\begin{align}\n",
    "    ReLU[\\alpha \\cdot z] &= \\frac{\\alpha \\cdot z + |\\alpha \\cdot z|}{2} \\\\\n",
    "    &= \\alpha \\cdot \\frac{z + |z|}{2} \\\\\n",
    "    &= \\alpha \\cdot ReLU[z]\n",
    "\\end{align}\n",
    "\n",
    "where we have used the fact that $\\alpha \\ge 0$ to extract it from the absolute value in line 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de9296-e65f-4121-a422-804c0f2e9b2c",
   "metadata": {},
   "source": [
    "### Problem 3.6 Following on from problem 3.5, what happens to the shallow network defined in equations 3.3 and 3.4 when we multiply the parameters θ10 and θ11 by a positive constant α and divide the slope ϕ1 by the same parameter α? What happens if α is negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3fa4c-3c5a-475d-b846-8d72d4b3f5d3",
   "metadata": {},
   "source": [
    "If alpha is positive, scaling the thetas by alpha and the corresponding slope by 1/alpha produces the same response. If alpha is negative, we get an entirely different response as multiplication with a negative constant does not commute with the ReLU operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cf895-2fb4-4370-90fc-fd81e2224873",
   "metadata": {},
   "source": [
    "### Problem 3.7 Consider fitting the model in equation 3.1 using a least squares loss function. Does this loss function have a unique minimum? i.e., is there a single “best” set of parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f829b3-9221-4545-9efa-8944526f2914",
   "metadata": {},
   "source": [
    "The model does not have a unique minimum.  As discussed in the previous problem, we can trade magnitude between the theta parameters and the corresponding phi parameter. This implies that for every model loss there is a continuous 3d manifold of equivalent solutions with exactly the same loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ef8f9-15dd-49c9-859d-9ce68263eacc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T21:46:18.237296Z",
     "iopub.status.busy": "2024-07-21T21:46:18.236969Z",
     "iopub.status.idle": "2024-07-21T21:46:18.241137Z",
     "shell.execute_reply": "2024-07-21T21:46:18.240545Z",
     "shell.execute_reply.started": "2024-07-21T21:46:18.237275Z"
    }
   },
   "source": [
    "### Problem 3.8 Consider replacing the ReLU activation function with (i) the Heaviside step function heaviside[z], (ii) the hyperbolic tangent function tanh[z], and (iii) the rectangular function rect[z], where: heaviside[z] = (0 z < 0 1 z ≥ 0 rect[z] = 0 z < 0 1 0 ≤ z ≤ 1 0 z > 1. (3.15) Redraw a version of figure 3.3 for each of these functions. The original parameters were: ϕ = {ϕ0, ϕ1, ϕ2, ϕ3, θ10, θ11, θ20, θ21, θ30, θ31} = {−0.23, −1.3, 1.3, 0.66, −0.2, 0.4, −0.9, 0.9, 1.1, −0.7}. Provide an informal description of the family of functions that can be created by neural networks with one input, three hidden units, and one output for each activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174505eb-2884-43a8-be7e-84b8ade0c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a shallow neural network with, one input, one output, and three hidden units\n",
    "def shallow_1_1_3(\n",
    "    x, \n",
    "    activation_fn, \n",
    "    phi_0,\n",
    "    phi_1,\n",
    "    phi_2,\n",
    "    phi_3, \n",
    "    theta_10, \n",
    "    theta_11, \n",
    "    theta_20, \n",
    "    theta_21, \n",
    "    theta_30, \n",
    "    theta_31,\n",
    "):\n",
    "  # TODO Replace the code below to compute the three initial lines\n",
    "  # from the theta parameters (i.e. implement equations at bottom of figure 3.3a-c).  These are the preactivations\n",
    "  pre_1 = theta_10 + theta_11*x\n",
    "  pre_2 = theta_20 + theta_21*x\n",
    "  pre_3 = theta_30 + theta_31*x  \n",
    "\n",
    "  # Pass these through the ReLU function to compute the activations as in\n",
    "  # figure 3.3 d-f\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  # TODO Replace the code below to weight the activations using phi1, phi2 and phi3\n",
    "  # To create the equivalent of figure 3.3 g-i\n",
    "  w_act_1 = phi_1 * act_1\n",
    "  w_act_2 = phi_2 * act_2\n",
    "  w_act_3 = phi_3 * act_3\n",
    "  \n",
    "  # TODO Replace the code below to combining the weighted activations and add\n",
    "  # phi_0 to create the output as in figure 3.3 j\n",
    "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3\n",
    "\n",
    "\n",
    "# Plot the shallow neural network.  We'll assume input in is range [0,1] and output [-1,1]\n",
    "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3\n",
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  if plot_all:\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')\n",
    "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')\n",
    "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')\n",
    "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')\n",
    "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')\n",
    "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')\n",
    "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')\n",
    "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')\n",
    "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,2]);ax[plot_x,plot_y].set_ylim([-1,1])        \n",
    "      ax[2,plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x,y)\n",
    "  ax.set_xlabel('Input, $x$'); ax.set_ylabel('Output, $y$')\n",
    "  ax.set_xlim([0,2]);ax.set_ylim([-1,1])  \n",
    "  if x_data is not None:\n",
    "    ax.plot(x_data, y_data, 'mo')\n",
    "    for i in range(len(x_data)):\n",
    "      ax.plot(x_data[i], y_data[i],)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_network(\n",
    "    activation_fn,\n",
    "    theta_10=-0.2, theta_11=0.4,\n",
    "    theta_20=-0.9, theta_21=0.9,\n",
    "    theta_30=1.1, theta_31=-0.7,\n",
    "    phi_0=-0.23, phi_1=-1.3, phi_2=1.3, phi_3=0.66,\n",
    "    plot_all=True\n",
    "):    \n",
    "    # Define a range of input values\n",
    "    x = np.arange(0,2,0.01)\n",
    "    \n",
    "    # We run the neural network for each of these input values\n",
    "    y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "        shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "    # And then plot it\n",
    "    plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=plot_all)\n",
    "\n",
    "def relu(z):\n",
    "    return z.clip(0)\n",
    "\n",
    "def heaviside(z):\n",
    "    return (z >= 0).astype(float)\n",
    "\n",
    "def rect(z):\n",
    "    return ((0 <= z) & (z <= 1)).astype(float)\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "    \n",
    "\n",
    "run_network(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5916d22-9c3d-4eda-aa1c-c6120e4a3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_network(heaviside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2ed9d-6b72-4966-8b46-5f920033757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_network(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7d8c6-e6d5-4943-876e-69cc9fbc9e93",
   "metadata": {},
   "source": [
    "### Problem 3.9∗ Show that the third linear region in figure 3.3 has a slope that is the sum of the slopes of the first and fourth linear regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc7f90-1cf8-406f-ae33-5df3bf51f9c5",
   "metadata": {},
   "source": [
    "This follows directly from 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd9291-fba3-40db-9913-572c1a554d69",
   "metadata": {},
   "source": [
    "### Problem 3.10 Consider a neural network with one input, one output, and three hidden units.The construction in figure 3.3 shows how this creates four linear regions. Under what circumstances could this network produce a function with fewer than four linear regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41018d2b-7310-419c-838a-3e70d6a8eb80",
   "metadata": {},
   "source": [
    "This was explored in notebook 3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962d6aa-c88e-49c4-9ca2-fad9f10ccc15",
   "metadata": {},
   "source": [
    "### Problem 3.11∗ How many parameters does the model in figure 3.6 have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc2c18-1994-44a6-a747-7ddd85db6966",
   "metadata": {},
   "source": [
    "$D_i = 1$\n",
    "\n",
    "$D_h = 4$\n",
    "\n",
    "$D_o = 2$\n",
    "\n",
    "$D_i \\rightarrow D_h = (D_i + 1) * D_h = (1 + 1) * 4 = 8$\n",
    "\n",
    "$D_h \\rightarrow D_o = (D_h + 1) * D_o = (4 + 1) * 2 = 10$\n",
    "\n",
    "So 18 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95f7a6-fac1-4fe1-a07d-2175a4b8b5df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T22:01:22.755405Z",
     "iopub.status.busy": "2024-07-21T22:01:22.755064Z",
     "iopub.status.idle": "2024-07-21T22:01:22.758801Z",
     "shell.execute_reply": "2024-07-21T22:01:22.758125Z",
     "shell.execute_reply.started": "2024-07-21T22:01:22.755382Z"
    }
   },
   "source": [
    "### Problem 3.12 How many parameters does the model in figure 3.7 have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3e13b-f836-4516-9682-2bf648add63d",
   "metadata": {},
   "source": [
    "$D_i = 2$\n",
    "\n",
    "$D_h = 3$\n",
    "\n",
    "$D_o = 1$\n",
    "\n",
    "$D_i \\rightarrow D_h = (D_i + 1) * D_h = (2 + 1) * 3 = 9$\n",
    "\n",
    "$D_h \\rightarrow D_o = (D_h + 1) * D_o = (3 + 1) * 1 = 3$\n",
    "\n",
    "So 12 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43cd7a-82ed-4c01-89af-ee73e9fba83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T22:01:50.185943Z",
     "iopub.status.busy": "2024-07-21T22:01:50.185630Z",
     "iopub.status.idle": "2024-07-21T22:01:50.189685Z",
     "shell.execute_reply": "2024-07-21T22:01:50.189041Z",
     "shell.execute_reply.started": "2024-07-21T22:01:50.185921Z"
    }
   },
   "source": [
    "### Problem 3.13 What is the activation pattern for each of the seven regions in figure 3.8? In other words, which hidden units are active (pass the input) and which are inactive (clip the input) for each region?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7da2ab-0628-4012-bc89-159e05b56cdf",
   "metadata": {},
   "source": [
    "Using the unit 3 bar as a divisor and going left to right\n",
    "\n",
    "1. None\n",
    "2. 1\n",
    "3. 1 and 2\n",
    "4. 3\n",
    "5. 1 and 3 (middle)\n",
    "6. 1, 2 , and 3\n",
    "7. 2 and 3 (bottom middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e49f7a-6901-4101-9060-a43cdd30875d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T22:02:16.017339Z",
     "iopub.status.busy": "2024-07-21T22:02:16.017038Z",
     "iopub.status.idle": "2024-07-21T22:02:16.021153Z",
     "shell.execute_reply": "2024-07-21T22:02:16.020455Z",
     "shell.execute_reply.started": "2024-07-21T22:02:16.017318Z"
    }
   },
   "source": [
    "### Problem 3.14 Write out the equations that define the network in figure 3.11. There should be three equations to compute the three hidden units from the inputs and two equations to compute the outputs from the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd885c6-bdde-419b-bf11-0d4df9cca8c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T22:46:02.913387Z",
     "iopub.status.busy": "2024-07-21T22:46:02.913099Z",
     "iopub.status.idle": "2024-07-21T22:46:02.918019Z",
     "shell.execute_reply": "2024-07-21T22:46:02.917074Z",
     "shell.execute_reply.started": "2024-07-21T22:46:02.913367Z"
    }
   },
   "source": [
    "\n",
    "\\begin{align}\n",
    "    h_1 &= a[\\theta_{10} + \\theta_{11} x_1 + \\theta_{12} x_2 + \\theta_{13} x_3] \\\\\n",
    "    h_2 &= a[\\theta_{20} + \\theta_{21} x_1 + \\theta_{22} x_2 + \\theta_{23} x_3] \\\\\n",
    "    h_3 &= a[\\theta_{30} + \\theta_{31} x_1 + \\theta_{32} x_2 + \\theta_{33} x_3] \\\\\n",
    "    y_1 &= \\phi_{10} + \\phi_{11} h_1 + \\phi_{12} h_2 + \\phi_{13} h_3 \\\\\n",
    "    y_2 &= \\phi_{20} + \\phi_{21} h_1 + \\phi_{22} h_2 + \\phi_{23} h_3 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a52aa-657b-4522-b136-400eec042554",
   "metadata": {},
   "source": [
    "### Problem 3.15∗ What is the maximum possible number of 3D linear regions that can be created by the network in figure 3.11?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52111683-6f65-44e0-addc-295b657187bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T22:47:29.828126Z",
     "iopub.status.busy": "2024-07-21T22:47:29.827851Z",
     "iopub.status.idle": "2024-07-21T22:47:29.832821Z",
     "shell.execute_reply": "2024-07-21T22:47:29.831835Z",
     "shell.execute_reply.started": "2024-07-21T22:47:29.828106Z"
    }
   },
   "source": [
    "The number of dimensions goes as \n",
    "\n",
    "\\begin{align}\n",
    "    N &= \\sum\\limits_{i=0}^{D_i} {D \\choose i} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Here we have D_i = 3 and D = 3 so we get \n",
    "\n",
    "\\begin{align}\n",
    "    {3 \\choose 0} + {3 \\choose 1} + {3 \\choose 2} + {3 \\choose 3} = 1 + 3 + 3 + 1 = 8\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c9af9-cccc-463c-8518-7bf04c5cb6cf",
   "metadata": {},
   "source": [
    "### Problem 3.16 Write out the equations for a network with two inputs, four hidden units, and three outputs. Draw this model in the style of figure 3.11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7739eb4-335b-41e6-8111-cbc72be13f3a",
   "metadata": {},
   "source": [
    "Pass, this is not helpful at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599ca1c-648e-4787-9e3b-3692ff72f00e",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 3.17∗ Equations 3.11 and 3.12 define a general neural network with Di inputs, one hidden layer containing D hidden units, and Do outputs. Find an expression for the number of parameters in the model in terms of Di, D, and Do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d772151-35e6-4661-a82a-7a85d6ac597f",
   "metadata": {},
   "source": [
    "Parameters between the input layer and the hidden layer are \n",
    "\n",
    "$D_i \\rightarrow D_h = (D_i + 1) * D$\n",
    "\n",
    "and then between the hidden layer and the output layer are \n",
    "\n",
    "$D \\rightarrow D_o = (D + 1) * D_o$\n",
    "\n",
    "so in total we have\n",
    "\n",
    "$Params = (D_i + 1)*D + (D + 1)*D_o$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24432902-d32a-4899-898e-450f0214aaff",
   "metadata": {},
   "source": [
    "### Problem 3.18∗ Show that the maximum number of regions created by a shallow network with Di = 2-dimensional input, Do = 1-dimensional output, and D = 3 hidden units is seven, as in figure 3.8j. Use the result of Zaslavsky (1975) that the maximum number of regions created by partitioning a Di-dimensional space with D hyperplanes is PDi j=0 Dj. What is the maximum number of regions if we add two more hidden units to this model, so D = 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb30756-eccc-4450-bd44-de46d26f1622",
   "metadata": {},
   "source": [
    "For $D_i = 2$ and $D = 3$ we have \n",
    "\n",
    "\\begin{align}\n",
    "    {3 \\choose 0} + {3 \\choose 1} + {3 \\choose 2} = 1 + 3 + 3 = 7\n",
    "\\end{align}\n",
    "\n",
    "With $D = 5$ hidden units we get a max of \n",
    "\n",
    "\\begin{align}\n",
    "    {5 \\choose 0} + {5 \\choose 1} + {5 \\choose 2} = 1 + 5 + 10 = 16\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503514f8-6424-466e-a8f9-4adb4fe228bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
