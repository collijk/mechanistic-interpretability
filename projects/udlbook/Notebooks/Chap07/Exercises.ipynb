{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345870cb-0105-4e9f-8ead-3dced7f5fa82",
   "metadata": {},
   "source": [
    "### Problem 7.1 A two-layer network with two hidden units in each layer can be defined as: y = ϕ0 + ϕ1ahψ01 + ψ11a[θ01 + θ11x] + ψ21a[θ02 + θ12x] i +ϕ2a h ψ02 + ψ12a[θ01 + θ11x] + ψ22a[θ02 + θ12x] i , (7.34) where the functions a[•] are ReLU functions. Compute the derivatives of the output y with respect to each of the 13 parameters ϕ• , θ••, and ψ•• directly (i.e., not using the backpropagationalgorithm). The derivative of the ReLU function with respect to its input ∂a[z]/∂z is theindicator function I[z > 0], which returns one if the argument is greater than zero and zero otherwise (figure 7.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45100a05-5602-4b3d-92c8-edcef415d8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fbdab30-a7e8-429f-8c46-e6a7b4b2b1c5",
   "metadata": {},
   "source": [
    "### Problem 7.2 Find an expression for the final term in each of the five chains of derivatives in equation 7.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1bf8b-24d9-4a70-b22f-124c2303b697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdcd96c-3276-4878-b6c0-9779f7370280",
   "metadata": {},
   "source": [
    "### Problem 7.3 What size are each of the terms in equation 7.19?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aac1a6-5443-456b-b076-ecb699248eae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1421c3df-ec8a-4c85-811e-5fb0daa8eefc",
   "metadata": {},
   "source": [
    "### Problem 7.4 Calculate the derivative ∂ℓi/∂f[xi, ϕ] for the least squares loss function: ℓi = (yi − f[xi, ϕ])2. (7.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a7579-9f93-4aea-8e99-fb4f94a4a27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f393085-f055-49ff-bd00-ba978b25532f",
   "metadata": {},
   "source": [
    "### Problem 7.5 Calculate the derivative ∂ℓi/∂f[xi, ϕ] for the binary classification loss function: ℓi = −(1 − yi) logh1 − sigf[xi, ϕ] i − yi logh sigf[xi, ϕ]i, (7.36) where the function sig[•] is the logistic sigmoid and is defined as: sig[z] = 1 1 + exp[−z]. (7.37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78b289-8f13-4e1f-a14d-004a8bfe3f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d88c1d8b-2c02-41f6-9cf4-63aaf9650cb6",
   "metadata": {},
   "source": [
    "### Problem 7.6∗ Show that for z = β + Ωh: ∂z ∂h = Ω T , where ∂z/∂h is a matrix containing the term ∂zi/∂hj in its i th column and j th row. To do this, first find an expression for the constituent elements ∂zi/∂hj , and then consider the form that the matrix ∂z/∂h must take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3255c-6b77-4743-a07c-f9f3c11e0363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54d63dae-d6ed-4783-9121-ac510086f78f",
   "metadata": {},
   "source": [
    "### Problem 7.7 Consider the case where we use the logistic sigmoid (see equation 7.37) as an activation function, so h = sig[f]. Compute the derivative ∂h/∂f for this activation function. What happens to the derivative when the input takes (i) a large positive value and (ii) a large negative value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe951b1-b3ff-45a4-b5b4-a67a68472bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5703a5ac-0340-42f4-acc7-dddc9b9a362d",
   "metadata": {},
   "source": [
    "### Problem 7.8 Consider using (i) the Heaviside function and (ii) the rectangular function as activation functions: Heaviside[z] = ( 0 z < 01 z ≥ 0, (7.38) and rect[z] = 0 z < 01 0 ≤ z ≤ 10 z > 1. (7.39) Discuss why these functions are problematic for neural network training with gradient-based optimization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ca44e-2ed4-40ac-8580-712fd20d0526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b74adc63-e27d-4777-8bd9-34e923a0815b",
   "metadata": {},
   "source": [
    "### Problem 7.9∗ Consider a loss function ℓ[f], where f = β + Ωh. We want to find how the loss ℓ changes when we change Ω, which we’ll express with a matrix that contains the derivative∂ℓ/∂Ωij at the ith row and jth column. Find an expression for ∂fi/∂Ωij and, using the chain rule, show that:∂ℓ∂Ω=∂ℓ∂fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df976a46-28a5-48a9-a9b6-501bc8a01e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639eff73-5a91-40b6-83b3-6ef3f8bf16f9",
   "metadata": {},
   "source": [
    "### Problem 7.10∗ Derive the equations for the backward pass of the backpropagation algorithm for a network that uses leaky ReLU activations, which are defined as: a[z] = ReLU[z] = (α · z z < 0z z ≥ 0, (7.41) where α is a small positive constant (typically 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817a1ce-891c-44af-90a2-07a774f66b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8456026-3b2b-4ed7-a013-0c690533c3a3",
   "metadata": {},
   "source": [
    "### Problem 7.11 Consider training a network with fifty layers, where we only have enough memory to store the pre-activations at every tenth hidden layer during the forward pass. Explain how to compute the derivatives in this situation using gradient checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5acf6-449d-4d57-9d04-5f765330d9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f348154-4925-4f7e-a4f6-1b328125cacd",
   "metadata": {},
   "source": [
    "### Problem 7.12∗ This problem explores computing derivatives on general acyclic computational graphs. Consider the function: y = exp exp[x] + exp[x] 2 + sin[exp[x] + exp[x]2]. (7.42)We can break this down into a series of intermediate computations so that f1 = exp[x] f2 = f21 f3 = f1 + f2 f4 = exp[f3] f5 = sin[f3] y = f4 + f5. (7.43) The associated computational graph is depicted in figure 7.9. Compute the derivative ∂y/∂x by reverse-mode differentiation. In other words, compute in order: ∂y∂f5 , ∂y∂f4, ∂y∂f3, ∂y∂f2, ∂y∂f1 and ∂y∂x, (7.44) using the chain rule in each case to make use of the derivatives already computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fee4e8-312e-4443-83ad-c93903e529e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96410eb5-13ad-438c-8df5-89cc36924e86",
   "metadata": {},
   "source": [
    "### Problem 7.13∗ For the same function as in problem 7.12, compute the derivative ∂y/∂x by forward-mode differentiation. In other words, compute in order: ∂f1∂x,∂f2∂x,∂f3∂x,∂f4∂x,∂f5∂x, and ∂y∂x, 7.45) using the chain rule in each case to make use of the derivatives already computed. Why do we not use forward-mode differentiation when we calculate the parameter gradients for deep networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37473f3-b423-4939-8e6a-6a78dddb4889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370070ed-4546-4094-90d3-1578e25eda70",
   "metadata": {},
   "source": [
    "### Problem 7.14 Consider a random variable a with variance Var[a] = σ2and a symmetrical distribution around the mean E[a] = 0. Prove that if we pass this variable through the ReLU function: b = ReLU[a] = (0 a < 0 a a ≥ 0 , (7.46) then the second moment of the transformed variable is E[b2] = σ2/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32b70a-aa6f-4963-98a5-734fdac3fb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203ea306-9302-4cda-a3ad-e905781acf59",
   "metadata": {},
   "source": [
    "### Problem 7.15 What would you expect to happen if we initialized all of the weights and biases in the network to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edf5e8-1a1c-4dc9-9fab-20be0816fc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62211ebd-0b71-4601-bdd7-0485aae88baa",
   "metadata": {},
   "source": [
    "### Problem 7.16 Implement the code in figure 7.8 in PyTorch and plot the training loss as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78e8e7-1d88-4358-b9b5-c0c722fc5725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dea976-2a27-4295-856e-c676e92f7bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd55db5-cac6-4eaf-93a6-94e840d907f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c25410-b988-4747-9cb0-fdb8d0549eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
